{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77875efa-4d18-41e1-b84e-94503daccfea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional\n",
    "\n",
    "import mlflow\n",
    "import uuid\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "multi_agent_config = mlflow.models.ModelConfig(development_config=\"smoke_test_config.yaml\")\n",
    "catalog = multi_agent_config.get(\"databricks_config\").get(\"catalog\")\n",
    "schema = multi_agent_config.get(\"databricks_config\").get(\"schema\")\n",
    "\n",
    "tools = []\n",
    "\n",
    "###################################################\n",
    "## Create a GenieAgent with access to a Genie Space\n",
    "###################################################\n",
    "\n",
    "GENIE_SPACE_ID = multi_agent_config.get(\"genie_space_config\").get(\"genie_space_id\")\n",
    "genie_agent_description = multi_agent_config.get(\"genie_space_config\").get(\"genie_space_description\")\n",
    "\n",
    "genie_agent = GenieAgent(\n",
    "    genie_space_id=GENIE_SPACE_ID,\n",
    "    genie_agent_name=\"Transactions Genie\",\n",
    "    description=genie_agent_description,\n",
    "    # DB_MODEL_SERVING_HOST_URL is set on an agent endpoints but doesn't exist in the notebook\n",
    "    client=WorkspaceClient(\n",
    "        host=os.getenv(\"DATABRICKS_HOST\") or os.getenv(\"DB_MODEL_SERVING_HOST_URL\"),\n",
    "        token=os.getenv(\"DATABRICKS_GENIE_PAT\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = multi_agent_config.get(\"agent_config\").get(\"llm_endpoint_name\")\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "index_name = multi_agent_config.get(\"vector_search_config\").get(\"vector_search_index_name\")\n",
    "\n",
    "vector_search_tools = [\n",
    "        VectorSearchRetrieverTool(\n",
    "        index_name=f\"{catalog}.{schema}.{index_name}\",\n",
    "        tool_description=multi_agent_config.get(\"vector_search_config\").get(\"vector_search_tool_description\")\n",
    "    )\n",
    "]\n",
    "tools.extend(vector_search_tools)\n",
    "\n",
    "vs_agent_description = multi_agent_config.get(\"vector_search_config\").get(\"vector_search_tool_description\")\n",
    "vs_agent = create_react_agent(llm, tools=tools)\n",
    "\n",
    "#############################\n",
    "# Define the supervisor agent\n",
    "#############################\n",
    "\n",
    "# TODO update the max number of iterations between supervisor and worker nodes\n",
    "# before returning to the user\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "worker_descriptions = {\n",
    "    \"Genie\": genie_agent_description,\n",
    "    \"Retriever\": vs_agent_description,\n",
    "}\n",
    "\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "system_prompt = multi_agent_config.get(\"agent_config\").get(\"system_prompt\").format(\n",
    "    formatted_descriptions=formatted_descriptions)\n",
    "\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    print('iteration count', count)\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return {\"next_node\": \"FINISH\"}\n",
    "    \n",
    "    class nextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | llm.with_structured_output(nextNode)\n",
    "    next_node = supervisor_chain.invoke(state).next_node\n",
    "    print(type(next_node), next_node)\n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Define our multiagent graph structure\n",
    "#######################################\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": name,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def final_answer(state):\n",
    "    system_prompt = \"Using only the content in the messages, respond to the user's question using the answer given by the other agents.\"\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | llm\n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "\n",
    "class AgentState(ChatAgentState):\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "\n",
    "\n",
    "code_node = functools.partial(agent_node, agent=vs_agent, name=\"Retriever\")\n",
    "genie_node = functools.partial(agent_node, agent=genie_agent, name=\"Genie\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Genie\", genie_node)\n",
    "workflow.add_node(\"Retriever\", code_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# Let the supervisor decide which next node to go\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "multi_agent = workflow.compile()\n",
    "\n",
    "###################################\n",
    "# Wrap our multi-agent in ChatAgent\n",
    "###################################\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        response_id = str(uuid.uuid4())\n",
    "        \n",
    "        for event in self.agent.stream(request, stream_mode=\"messages\"):\n",
    "            # Event is a tuple: (AIMessageChunk, metadata)\n",
    "            if isinstance(event, tuple) and len(event) >= 2:\n",
    "                message_chunk, metadata = event[0], event[1]\n",
    "                # Extract content from AIMessageChunk\n",
    "                content = message_chunk.content\n",
    "                idid = message_chunk.id\n",
    "                # AIMessageChunk typically doesnâ€™t have role in stream_mode=\"messages\", default to \"assistant\"\n",
    "                role = getattr(message_chunk, \"role\", \"assistant\") if hasattr(message_chunk, \"role\") else \"assistant\"\n",
    "            else:\n",
    "                print(\"Unexpected event format:\", event)\n",
    "                continue\n",
    "            \n",
    "            if not content:  # Skip empty chunks\n",
    "                continue\n",
    "\n",
    "            # response_id = str(uuid.uuid4())\n",
    "\n",
    "            chunk = ChatAgentChunk(\n",
    "                delta=ChatAgentMessage(\n",
    "                        **{\n",
    "                            \"role\": role,\n",
    "                            \"content\": content,\n",
    "                            \"id\": response_id,\n",
    "                        }\n",
    "                    )\n",
    "            )\n",
    "            yield chunk\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphChatAgent(multi_agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02a_rag_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

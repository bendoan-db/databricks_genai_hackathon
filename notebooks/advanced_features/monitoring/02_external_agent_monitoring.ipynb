{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a5c5b1-0fa7-499f-a4a9-f83219c4c435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# External monitor setup\n",
    "\n",
    "This Notebook is designed to run from a Databricks workspace.  See the [documentation](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring-non-agent-framework) for a `.py` version that runs from your local IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f39e7d3-b1e6-4c0d-80e1-6af211ae2c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U \"databricks-agents>=0.18.1\" \"mlflow>=2.21.2\" \"databricks-sdk[openai]\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe4397e-2226-4fc2-aeac-bcb9daeee13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Configure the MLflow Experiment where your monitor will live.\n",
    "\n",
    "The monitoring UI will appear in as a tab in the MLflow Experiment and the ACL to your traces is controlled through the MLflow experiment's ACLs.  In your app's production code, you will reference this experiment_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab7ef277-b206-4631-b551-974335eaf9d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Get the current user name\n",
    "user_email = spark.sql(\"SELECT current_user() as username\").collect()[0].username\n",
    "user = user_email.split(\"@\")[0].replace(\".\", \"\").lower()[:35]\n",
    "\n",
    "# For convinience, we auto-infer these from the current notebook.\n",
    "# TODO: In prod and outside notebooks, set these explicitly.\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"my_app_monitor\"\n",
    "MLFLOW_EXPERIMENT_PATH = f\"/Users/{user_email}/{MLFLOW_EXPERIMENT_NAME}\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_PATH)\n",
    "MLFLOW_EXPERIMENT_ID = mlflow.tracking.fluent._get_experiment_id() \n",
    "\n",
    "# Get the current Databricks workspace URL\n",
    "DATABRICKS_HOST = mlflow.utils.databricks_utils.get_workspace_url() # https://<workspace-url>.databricks.com\n",
    "\n",
    "print(f\"MLFLOW_EXPERIMENT_ID: {MLFLOW_EXPERIMENT_ID}\")\n",
    "print(f\"MLFLOW_EXPERIMENT_PATH: {MLFLOW_EXPERIMENT_PATH}\")\n",
    "print(f\"DATABRICKS_HOST: {DATABRICKS_HOST}\")\n",
    "\n",
    "print(f\"\\nView the monitoring UI in the MLflow experiment here: {DATABRICKS_HOST}/ml/experiments/{MLFLOW_EXPERIMENT_ID}/evaluation-monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b38a6a3d-daf8-455d-8c7e-420fa8b26bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the monitor.\n",
    "\n",
    "For details on the parameters, please see the [create a monitor](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring-non-agent-framework#step-1-create-an-external-monitor) documentation.  You should customize the metrics based on your business requirements.\n",
    "\n",
    "To configure a monitor, you must provide a catalog and schema where you have CREATE TABLE permissions.  The monitoring job creates and uses a checkpoint table in this catalog/schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7960824-b773-42dd-8d6d-ef3e673a3e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the workspace default UC catalog / schema\n",
    "uc_default_location = spark.sql(\"select current_catalog() as current_catalog, current_schema() as current_schema\").collect()[0]\n",
    "current_catalog = uc_default_location[\"current_catalog\"]\n",
    "current_schema = uc_default_location[\"current_schema\"]\n",
    "\n",
    "\n",
    "# Modify the UC catalog / schema here or at the top of the notebook in the widget editor\n",
    "dbutils.widgets.text(\"uc_catalog\", current_catalog)\n",
    "dbutils.widgets.text(\"uc_schema\", current_schema)\n",
    "UC_CATALOG = dbutils.widgets.get(\"uc_catalog\")\n",
    "UC_SCHEMA = dbutils.widgets.get(\"uc_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e822e35-727d-4f53-95a4-57a411f7ed40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.monitoring import create_external_monitor, AssessmentsSuiteConfig, BuiltinJudge, GuidelinesJudge, get_external_monitor\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "try:\n",
    "  external_monitor = create_external_monitor(\n",
    "    catalog_name=UC_CATALOG,\n",
    "    schema_name=UC_SCHEMA,\n",
    "    experiment_id=MLFLOW_EXPERIMENT_ID,\n",
    "    assessments_config=AssessmentsSuiteConfig(\n",
    "      # The % of traces on which the `assessments` are run; a number between 0 and 1.\n",
    "      sample=1.0,\n",
    "      assessments=[\n",
    "        # Builtin judges: \"safety\", \"groundedness\", \"relevance_to_query\", \"chunk_relevance\"\n",
    "        BuiltinJudge(name='safety'),\n",
    "        BuiltinJudge(name='groundedness'),\n",
    "        BuiltinJudge(name='relevance_to_query'),\n",
    "        BuiltinJudge(name='chunk_relevance'),\n",
    "        # Create custom LLM judges with the guidelines judge.\n",
    "        GuidelinesJudge(guidelines={\n",
    "          \"pii\": [\"The response must not contain personal information.\"],\n",
    "          \"english\": [\"The response must be in English\"]\n",
    "        }),\n",
    "      ]\n",
    "    ))\n",
    "except HTTPError as e:\n",
    "    if e.response.status_code == 409 and \"ALREADY_EXISTS\" in e.response.text:\n",
    "        print(\"A monitor for this experiment already exists. Retrieving that monitor...\\n\\n\")\n",
    "        external_monitor = get_external_monitor(experiment_id=MLFLOW_EXPERIMENT_ID)\n",
    "        print(external_monitor)\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e610cd-d693-4944-b002-f2357054d5f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Now, instrument your agent/app's code with MLflow Tracing\n",
    "\n",
    "To instrument your agent:\n",
    "1. `pip install \"mlflow>=2.21.2\"`\n",
    "2. Set the `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables.\n",
    "    - `DATABRICKS_HOST` is your workspace's URL  e.g., `https://<workspace-url>.databricks.com`\n",
    "    - `DATABRICKS_TOKEN` is a PAT token.  Follow these [steps](https://docs.databricks.com/aws/en/dev-tools/auth/pat#pat-user).  \n",
    "       - If you want to use a service principal's PAT token, make sure to grant the service principal EDIT writes to the MLflow experiment you configured at the top of the notebook.  Without this, MLflow Tracing will NOT be able to log traces.\n",
    "3. Call `mlflow.tracing.set_destination(Databricks(experiment_id=MLFLOW_EXPERIMENT_ID))` to configure MLflow tracing to log to your monitor.\n",
    "    - `MLFLOW_EXPERIMENT_ID` is the ID of the MLflow experiment you created at the top of this Notebook.\n",
    "4. Instrument your agent using MLflow Tracing.  To learn how, see the [MLflow Tracing documentation](https://mlflow.org/docs/latest/tracing/). \n",
    "\n",
    "Here, we create a sample agent so you can test end to end that your monitor is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fe99f4-b18d-464d-b887-342f3fa336cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracing.destination import Databricks\n",
    "import os\n",
    "\n",
    "# In your production app, set these environment variables to enable MLflow Tracing to connect to your monitor.\n",
    "# os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST  # Your workspace's URL e.g., https://<workspace-url>.databricks.com\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN  # A PAT token\n",
    "\n",
    "# Use set_destination to configure MLflow tracing to log to your monitor.\n",
    "mlflow.tracing.set_destination(Databricks(experiment_id=MLFLOW_EXPERIMENT_ID))\n",
    "\n",
    "# Here, we use the databricks-sdk's convenience method to get an OpenAI client authenticated to your workspace.  For details of this convenience method, see https://docs.databricks.com/aws/en/machine-learning/model-serving/score-foundation-models#install-packages\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "openai_client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# In your app, you can use any OpenAI client.\n",
    "# import openai\n",
    "# openai_client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# Instrument your Agent using MLflow auto-logging or manual instrumentation.  Here we use a combination of auto-logging and manual instrumentation.\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Manual instrumentation\n",
    "@mlflow.trace(span_type=\"AGENT\")\n",
    "def openai_agent(user_input: str):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that always responds in CAPS!\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "openai_agent(\"What is GenAI observability?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f05876b-8f4e-4811-a452-271c1e7a0372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View the logged trace by going to the monitor's UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385c1cab-86c4-42df-aca9-653a90a40885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\\nView the monitoring UI in the MLflow experiment here: {DATABRICKS_HOST}/ml/experiments/{MLFLOW_EXPERIMENT_ID}/evaluation-monitoring?viewState=logs\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_external_agent_monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

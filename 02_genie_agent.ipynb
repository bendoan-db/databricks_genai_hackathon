{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8540db-034a-4573-b838-4aa6ed1035c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import mlflow\n",
    "import os\n",
    "from typing import Any, Callable, Dict, Generator, List, Optional\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "from vector_search_utils.self_querying_retriever import load_self_querying_retriever\n",
    "#from supervisor_utils.decomposer import load_decomposer\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain import DatabricksVectorSearch\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742ca599-f559-423a-93fc-49847b6ba2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Enable MLflow Tracing\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=\"./configs/agent.yaml\")\n",
    "\n",
    "databricks_config = model_config.get(\"databricks_config\")\n",
    "\n",
    "doc_agent_config = model_config.get(\"doc_agent_config\")\n",
    "genie_agent_config = model_config.get(\"genie_agent_config\")\n",
    "supervisor_config = model_config.get(\"supervisor_agent_config\")\n",
    "\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "\n",
    "doc_retrieval_model = ChatDatabricks(\n",
    "    endpoint=doc_agent_config.get(\"llm_config\").get(\"llm_endpoint_name\"),\n",
    "    extra_params=doc_agent_config.get(\"llm_config\").get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "genie_agent_model = ChatDatabricks(\n",
    "    endpoint=genie_agent_config.get(\"llm_config\").get(\"llm_endpoint_name\"),\n",
    "    extra_params=genie_agent_config.get(\"llm_config\").get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "supervisor_model = ChatDatabricks(\n",
    "    endpoint=supervisor_config.get(\"llm_config\").get(\"llm_endpoint_name\"),\n",
    "    extra_params=supervisor_config.get(\"llm_config\").get(\"llm_parameters\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b843b1-2d1b-4073-9bdd-e3afcc81873b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from supervisor_utils.handoff_tools import create_handoff_tool\n",
    "\n",
    "sq_retriever = load_self_querying_retriever(doc_retrieval_model, databricks_config, retriever_config)\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=doc_retrieval_model,\n",
    "    tools=[sq_retriever.as_tool()],\n",
    "    prompt=doc_agent_config.get(\"prompt\"),\n",
    "    name=\"research_agent\",\n",
    ")\n",
    "\n",
    "# Genie agent\n",
    "class GenieInput(BaseModel):\n",
    "    \"\"\"OpenAI-format chat history for Genie.\"\"\"\n",
    "    messages: List[Dict[str, Any]] = Field(\n",
    "        ..., description=\"Chat messages in {'role','content'} format\"\n",
    "    )\n",
    "\n",
    "genie_agent_tool = GenieAgent(\n",
    "    genie_space_id=genie_agent_config.get(\"genie_space_id\"),\n",
    "    genie_agent_name=\"sec_metrics_analyst\",\n",
    "    description=genie_agent_config.get(\"genie_space_description\"),\n",
    "    client=WorkspaceClient(),\n",
    ").as_tool(\n",
    "    name=\"sec_metrics_analyst\",\n",
    "    description=\"Ask Genie questions about public-company filings and metrics\",\n",
    "    args_schema=GenieInput,\n",
    ")\n",
    "\n",
    "genie_agent = create_react_agent(\n",
    "    model=genie_agent_model,\n",
    "    tools=[genie_agent_tool],\n",
    "    prompt=genie_agent_config.get(\"prompt\"),\n",
    "    name=\"sec_metrics_analyst\",\n",
    ")\n",
    "\n",
    "# Handoffs\n",
    "assign_to_research_agent = create_handoff_tool(\n",
    "    agent_name=\"research_agent\",\n",
    "    description=\"Assign task to the doc researcher agent.\",\n",
    ")\n",
    "\n",
    "# Handoffs\n",
    "assign_to_analyst_agent = create_handoff_tool(\n",
    "    agent_name=\"sec_metrics_analyst\",\n",
    "    description=\"Assign task to the sec metrics agent.\",\n",
    ")\n",
    "\n",
    "supervisor_agent = create_react_agent(\n",
    "    model=supervisor_model,\n",
    "    tools=[assign_to_research_agent, assign_to_analyst_agent],\n",
    "    prompt=supervisor_config.get(\"prompt\"),\n",
    "    name=\"supervisor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11a1251-243a-45ad-994f-877ce2bca069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.graph import StateGraph, START, MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "# Define the multi-agent supervisor graph\n",
    "supervisor = (\n",
    "    StateGraph(MessagesState)\n",
    "    # NOTE: `destinations` is only needed for visualization and doesn't affect runtime behavior\n",
    "    .add_node(supervisor_agent, destinations=(\"research_agent\", \"sec_metrics_analyst\", END))\n",
    "    .add_node(research_agent)\n",
    "    .add_node(genie_agent)\n",
    "    .add_edge(START, \"supervisor\")\n",
    "    # always return back to the supervisor\n",
    "    .add_edge(\"research_agent\", \"supervisor\")\n",
    "    .add_edge(\"sec_metrics_analyst\", \"supervisor\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "162638c0-368d-41d4-a526-c481c1a9506f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import SpanType\n",
    "\n",
    "\n",
    "class SECAgent(ChatAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def get_last_valid_message(self, messages: list):\n",
    "        for message in reversed(messages):\n",
    "            if message.content != \"\":\n",
    "                return message\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \n",
    "        if type(messages[0]) == mlflow.types.agent.ChatAgentMessage:\n",
    "            request = {\"messages\": [m.model_dump() for m in messages]}\n",
    "\n",
    "        agent_response = self.agent.invoke(request)\n",
    "\n",
    "        last_valid_agent_message = self.get_last_valid_message(agent_response[\"messages\"])\n",
    "\n",
    "        response = [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"id\": last_valid_agent_message.id,\n",
    "                \"content\": last_valid_agent_message.content,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return ChatAgentResponse(messages=response)\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "\n",
    "        request = {\"messages\": [m.model_dump() for m in messages]}\n",
    "\n",
    "        for event in self.agent.stream(request):\n",
    "            if len(event[\"messages\"][-1].content) > 0:\n",
    "                yield ChatAgentChunk(\n",
    "                    delta=ChatAgentMessage(\n",
    "                        content=event[\"messages\"][-1].content,\n",
    "                        role=\"assistant\",\n",
    "                        id=event[\"messages\"][-1].id,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "## Tell MLflow logging where to find your chain.\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = SECAgent(supervisor)\n",
    "\n",
    "mlflow.models.set_model(AGENT)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_genie_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8540db-034a-4573-b838-4aa6ed1035c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import mlflow\n",
    "import os\n",
    "from typing import Any, Callable, Dict, Generator, List, Optional\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "from vector_search_utils.self_querying_retriever import load_self_querying_retriever\n",
    "#from supervisor_utils.decomposer import load_decomposer\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain import DatabricksVectorSearch\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742ca599-f559-423a-93fc-49847b6ba2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=\"./configs/rag_agent.yaml\")\n",
    "\n",
    "databricks_config = model_config.get(\"databricks_config\")\n",
    "llm_config = model_config.get(\"llm_config\")\n",
    "\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "\n",
    "# FM for generation\n",
    "model = ChatDatabricks(\n",
    "    endpoint=llm_config.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b843b1-2d1b-4073-9bdd-e3afcc81873b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "sq_retriever = load_self_querying_retriever(model, databricks_config, retriever_config)\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[sq_retriever.as_tool()],\n",
    "    prompt=(\n",
    "        \"You are a research agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"research_agent\",\n",
    ")\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=model,\n",
    "    agents=[research_agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- An SEC research agent: This agent has access to SEC filings for specific companies. Task is to look up specific questions on an organization's strategy, risks, management decisions, and legal and financial discosures.\\n\"\n",
    "        \"- Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"- Once an agent has responded to you, check if there is more work to do. Call additional agents as needed to answer the question\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "162638c0-368d-41d4-a526-c481c1a9506f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SECAgent(ChatAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        agent_response = self.agent.invoke(request)\n",
    "\n",
    "        response = [{\"role\": \"assistant\", \"id\": agent_response[\"messages\"][-1].id, \"content\": agent_response[\"messages\"][-1].content}]\n",
    "\n",
    "        return ChatAgentResponse(messages=response)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        for event in self.agent.stream(request):\n",
    "            if len(event[\"messages\"][-1].content)>0:\n",
    "                yield ChatAgentChunk(\n",
    "                            delta=ChatAgentMessage(\n",
    "                                content=event[\"messages\"][-1].content,\n",
    "                                role=\"assistant\",\n",
    "                                id=event[\"messages\"][-1].id\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "## Tell MLflow logging where to find your chain.\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = SECAgent(supervisor)\n",
    "\n",
    "mlflow.models.set_model(AGENT)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01a_unstructured_retrieval_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

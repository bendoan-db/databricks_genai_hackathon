{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382afb01-077e-4221-96e6-2c9c4de958f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fake Jobs logs table for SQL tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "138c6d83-0ec4-48c3-9ecf-5455807272a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Créer la table Delta\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gportier_demo.spark_dock.spark_logs (\n",
    "        ID INT,\n",
    "        JobName STRING,\n",
    "        ErrorMessage STRING,\n",
    "        Timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db89802c-b926-40f6-9cc2-50455a1537f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, expr\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "job_names = [\"DataIngestion\", \"ETLProcess\", \"MLTraining\", \"ReportGeneration\", \"DataValidation\"]\n",
    "error_messages = [\n",
    "    \"NullPointerException at line 42\",\n",
    "    \"OutOfMemoryError: Java heap space\",\n",
    "    \"FileNotFoundException: /path/to/file\",\n",
    "    \"SQLException: Invalid query syntax\",\n",
    "    \"TimeoutException: Job took too long\"\n",
    "]\n",
    "\n",
    "data = []\n",
    "base_time = datetime(2025, 4, 7, 10, 0, 0)  \n",
    "for i in range(20):\n",
    "    row = {\n",
    "        \"ID\": i + 1,\n",
    "        \"JobName\": random.choice(job_names),\n",
    "        \"ErrorMessage\": random.choice(error_messages),\n",
    "        \"Timestamp\": base_time + timedelta(minutes=random.randint(0, 1440))  # Jusqu'à 24h plus tard\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "df = spark.createDataFrame(data, schema=\"ID INT, JobName STRING, ErrorMessage STRING, Timestamp TIMESTAMP\")\n",
    "\n",
    "df.write.mode(\"append\").format(\"delta\").saveAsTable(\"gportier_demo.spark_dock.spark_logs\")\n",
    "\n",
    "# Vérifier les données\n",
    "spark.sql(\"SELECT * FROM gportier_demo.spark_dock.spark_logs\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d920bed0-d76f-4d69-880a-86ac19019e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start building agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b71a91c-b03f-4e29-8557-eb6b443f79c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow langchain langgraph==0.3.4 databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65bc649-1d2d-4a48-b1ce-19ea00d35a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b49b74-021c-42a1-9eea-08051d280973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c2fb0b-7b17-43e5-aa71-13a69c9961b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agents/agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "import functools\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import ChatDatabricks, VectorSearchRetrieverTool\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents import create_sql_agent, create_react_agent\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME =  \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant. You can assist with help answiring questions about PySpark.\n",
    "Use tool 'SQLTools\" if user ask you about SQL query or analytics question.\n",
    "Use tools 'VectorSearch' if user ask you about a specific document or a specific page.\n",
    "\"\"\"\n",
    "\n",
    "tools = []\n",
    "\n",
    "\n",
    "claude=\"databricks-claude-3-7-sonnet\"\n",
    "llama31=\"databricks-meta-llama-3-1-70b-instruct\"\n",
    "db = SQLDatabase.from_databricks(catalog=\"gportier_demo\", schema=\"spark_dock\", engine_args={\"pool_pre_ping\": True,})\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "sql_agent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "vector_search_tools = [\n",
    "         VectorSearchRetrieverTool(\n",
    "         index_name=\"gportier_demo.rag_chatbot.databricks_documentation_vs_index\",\n",
    "         name=\"VectorSearchRetrieverTool\"  # Forcer le nom\n",
    "\n",
    "         # filters=\"...\"\n",
    "     )\n",
    " ]\n",
    "\n",
    "sql_tools = toolkit.get_tools()\n",
    "tools.extend(sql_tools)\n",
    "tools.extend(vector_search_tools)\n",
    "\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools,\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        \n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            # TODO: Is there a better way than listing all tools func?\n",
    "            tool_call = last_message[\"tool_calls\"][0]\n",
    "            tool_name = tool_call[\"function\"][\"name\"]\n",
    "            if tool_name in [\"sql_db_list_tables\", \"sql_db_schema\", \"sql_db_query\", \"sql_db_query_checker\"]:\n",
    "                return \"sql\"\n",
    "            elif tool_name == \"gportier_demo__rag_chatbot__databricks_documentation_vs_index\":\n",
    "                return \"vector\"\n",
    "            else:\n",
    "                print(f\"Unknown tool: {tool_name}\")\n",
    "                return \"end\"\n",
    "        return \"end\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": agent_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def add_custom_outputs(state: ChatAgentState):\n",
    "        # TODO: Return extra content with the custom_outputs key before returning\n",
    "        return {\n",
    "            \"custom_outputs\": {\n",
    "                **(state.get(\"custom_outputs\") or {}),\n",
    "                **(state.get(\"custom_inputs\") or {}),\n",
    "                \"key\": \"value\",\n",
    "            }, \n",
    "        }\n",
    "\n",
    "    \n",
    "        \n",
    "    def format_response(state: ChatAgentState):\n",
    "        # TODO DO BETTER\n",
    "        last_message = next((msg for msg in reversed(state[\"messages\"]) if msg[\"role\"] == \"assistant\"), None)\n",
    "        if last_message:\n",
    "            content = last_message[\"content\"]\n",
    "            formatted_content = f\"**{content}     !!!**\"\n",
    "            return {\n",
    "                \"messages\": [{\"role\": \"assistant\", \"content\": formatted_content}]\n",
    "            }\n",
    "        \n",
    "        return {\"messages\": []}  \n",
    "      \n",
    "    def format_response(state: ChatAgentState):\n",
    "        user_question = next((msg[\"content\"] for msg in reversed(state[\"messages\"]) if msg[\"role\"] == \"user\"), \"No question found\")\n",
    "        assistant_answer = next((msg[\"content\"] for msg in reversed(state[\"messages\"]) if msg[\"role\"] == \"assistant\"), \"No answer found\")\n",
    "        \n",
    "        markdown_template = f\"\"\"**{user_question}**\n",
    "\n",
    "                            # Answer\n",
    "                            **{assistant_answer}!!!**\n",
    "                            \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"messages\": [{\"role\": \"assistant\", \"content\": markdown_template}]\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"SQLTools\", ChatAgentToolNode(sql_tools))\n",
    "    workflow.add_node(\"VectoreSearch\", ChatAgentToolNode(vector_search_tools))\n",
    "    workflow.add_node(\"add_custom_outputs\", RunnableLambda(add_custom_outputs))\n",
    "    workflow.add_node(\"format_response\", RunnableLambda(format_response))  \n",
    "    \n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"sql\": \"SQLTools\",\n",
    "            \"vector\":\"VectoreSearch\",\n",
    "            \"end\": \"add_custom_outputs\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"SQLTools\", \"agent\")\n",
    "    workflow.add_edge(\"VectoreSearch\", \"agent\")\n",
    "\n",
    "    workflow.add_edge(\"add_custom_outputs\", \"format_response\")\n",
    "    workflow.add_edge(\"format_response\", END)\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # TODO: Use context and custom_inputs to alter the behavior of the agent\n",
    "        request = {\n",
    "            \"messages\": self._convert_messages_to_dict(messages),\n",
    "            **({\"custom_inputs\": custom_inputs} if custom_inputs else {}),\n",
    "            **({\"context\": context.model_dump_compat()} if context else {}),\n",
    "        }\n",
    "\n",
    "        response = ChatAgentResponse(messages=[])\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                if not node_data:\n",
    "                    continue\n",
    "                for msg in node_data.get(\"messages\", []):\n",
    "                    response.messages.append(ChatAgentMessage(**msg))\n",
    "                if \"custom_outputs\" in node_data:\n",
    "                    response.custom_outputs = node_data[\"custom_outputs\"]\n",
    "        return response\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        # TODO: Use context and custom_inputs to alter the behavior of the agent\n",
    "        request = {\n",
    "            \"messages\": self._convert_messages_to_dict(messages),\n",
    "            **({\"custom_inputs\": custom_inputs} if custom_inputs else {}),\n",
    "            **({\"context\": context.model_dump_compat()} if context else {}),\n",
    "        }\n",
    "\n",
    "        last_message = None\n",
    "        last_custom_outputs = None\n",
    "\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                if not node_data:\n",
    "                    continue\n",
    "                messages = node_data.get(\"messages\", [])\n",
    "                custom_outputs = node_data.get(\"custom_outputs\")\n",
    "\n",
    "                for message in messages:\n",
    "                    if last_message:\n",
    "                        yield ChatAgentChunk(delta=last_message)\n",
    "                    last_message = message\n",
    "                if custom_outputs:\n",
    "                    last_custom_outputs = custom_outputs\n",
    "        if last_message:\n",
    "            yield ChatAgentChunk(delta=last_message, custom_outputs=last_custom_outputs)\n",
    "\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2956984-5e9d-47c8-a523-659eb293fda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaea0928-c251-4823-8b46-8a085fbfb16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776c0694-128c-4a73-8fb2-4d3b158e50f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agents.agent import AGENT\n",
    "\n",
    "#AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"How many rows are in table databricks_documentation ?\"}]})\n",
    "#AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"How to add a column in pyspark ? \"}]})\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"I do have this error on my spark logs. SQLException: Invalid query syntax. Please tell me if some others deveveloppeur had same errors in our production spark logs \"}]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73932b1b-4d34-467c-8e00-aeb083f7ebc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agents.agent import AGENT,agent\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        agent.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a769363-1533-40fe-89fc-fa192063b7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 695471856099728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "sql-vector-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
